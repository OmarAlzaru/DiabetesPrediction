# -*- coding: utf-8 -*-
"""AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13o5fQQO8YiBRuZrPIDrruf0r74hBf-hE
"""

import warnings
warnings.filterwarnings('ignore')

# Import Neccessary libraries
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder


#Import Model
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.pipeline import Pipeline

#Import Sampler libraries
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as imbPipeline

# Set the decimal format
pd.options.display.float_format = "{:.2f}".format

df = pd.read_csv("diabetes_prediction_dataset.csv")

df.head()

df.describe()

df = df.drop_duplicates()

# Checking null values
print(df.isnull().sum())

# Remove Unneccessary value [0.00195%]
df = df[df['gender'] != 'Other']

# Define a function to map the existing categories to new ones
def recategorize_smoking(smoking_status):
    if smoking_status in ['never', 'No Info']:
        return 'non-smoker'
    elif smoking_status == 'current':
        return 'current'
    elif smoking_status in ['ever', 'former', 'not current']:
        return 'past_smoker'

# Apply the function to the 'smoking_history' column
df['smoking_history'] = df['smoking_history'].apply(recategorize_smoking)

# Check the new value counts
print(df['smoking_history'].value_counts())

# Define preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level','hypertension','heart_disease']),
        ('cat', OneHotEncoder(), ['gender','smoking_history'])
    ])

# Split data into features and target variable
X = df.drop('diabetes', axis=1)
y = df['diabetes']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.svm import SVC

# Define the SVM model
svm_model = SVC()

# Create a pipeline with preprocessor and SVM model
svm_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('svm', svm_model)
])

# Fit the pipeline on the training data
svm_pipeline.fit(X_train, y_train)

# Predict on the testing data
svm_predictions = svm_pipeline.predict(X_test)

# Evaluate the SVM model
svm_accuracy = accuracy_score(y_test, svm_predictions)
print("SVM Accuracy:", svm_accuracy)

from sklearn.tree import DecisionTreeClassifier

# Define the Decision Tree model
dt_model = DecisionTreeClassifier()

# Create a pipeline with preprocessor and Decision Tree model
dt_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('decision_tree', dt_model)
])

# Fit the pipeline on the training data
dt_pipeline.fit(X_train, y_train)

# Predict on the testing data
dt_predictions = dt_pipeline.predict(X_test)

# Evaluate the Decision Tree model
dt_accuracy = accuracy_score(y_test, dt_predictions)
print("Decision Tree Accuracy:", dt_accuracy)

from sklearn.neighbors import KNeighborsClassifier

# Define the KNN model
knn_model = KNeighborsClassifier()

# Create a pipeline with preprocessor and KNN model
knn_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('knn', knn_model)
])

# Fit the pipeline on the training data
knn_pipeline.fit(X_train, y_train)

# Predict on the testing data
knn_predictions = knn_pipeline.predict(X_test)

# Evaluate the KNN model
knn_accuracy = accuracy_score(y_test, knn_predictions)
print("KNN Accuracy:", knn_accuracy)

from sklearn.linear_model import LogisticRegression

# Define the Logistic Regression model
log_reg_model = LogisticRegression()

# Create a pipeline with preprocessor and Logistic Regression model
log_reg_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('log_reg', log_reg_model)
])

# Fit the pipeline on the training data
log_reg_pipeline.fit(X_train, y_train)

# Predict on the testing data
log_reg_predictions = log_reg_pipeline.predict(X_test)

# Evaluate the Logistic Regression model
log_reg_accuracy = accuracy_score(y_test, log_reg_predictions)
print("Logistic Regression Accuracy:", log_reg_accuracy)

from sklearn.ensemble import RandomForestClassifier

# Define the Random Forest model
rf_model = RandomForestClassifier()

# Create a pipeline with preprocessor and Random Forest model
rf_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('random_forest', rf_model)
])

# Fit the pipeline on the training data
rf_pipeline.fit(X_train, y_train)

# Predict on the testing data
rf_predictions = rf_pipeline.predict(X_test)

# Evaluate the Random Forest model
rf_accuracy = accuracy_score(y_test, rf_predictions)
print("Random Forest Accuracy:", rf_accuracy)

from sklearn.ensemble import GradientBoostingClassifier

# Define the GBM model
gbm_model = GradientBoostingClassifier()

# Create a pipeline with preprocessor and GBM model
gbm_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('gbm', gbm_model)
])

# Fit the pipeline on the training data
gbm_pipeline.fit(X_train, y_train)

# Predict on the testing data
gbm_predictions = gbm_pipeline.predict(X_test)

# Evaluate the GBM model
gbm_accuracy = accuracy_score(y_test, gbm_predictions)
print("GBM Accuracy:", gbm_accuracy)